{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Preprocessing :\n","\n","*   1) Hair removal\n","\n","*   2) Shade removal\n","\n","*   3) Contour detection\n","*   4) Histogram equalizer\n","*   5) Débruitage\n","*   6) Détection de symétrie\n","\n"],"metadata":{"id":"wSYthE6wYcKX"}},{"cell_type":"markdown","source":["**Hair Removal**"],"metadata":{"id":"BYFEboS4NaTb"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"dGGrkhj3NSYg","executionInfo":{"status":"error","timestamp":1687338491433,"user_tz":-120,"elapsed":423,"user":{"displayName":"Ethan B","userId":"16227102091835796664"}},"outputId":"e5b6614e-6749-4cbd-ad23-90e200d4a4d9"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-53bff3132efa>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Image cropping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m410\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m560\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# DULL RAZOR (REMOVE HAIR)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["import cv2\n","\n","\"\"\"\n","Most images of skin lesions have unwanted elements, such as shadows and hairs, which can make it difficult to segment\n","the lesion and introduce erroneous information on its characteristics. Therefore, it is necessary to apply some\n","artificial vision techniques to eliminate any noise component.\n","\n","Body hair is one of the factors that can affect lesion segmentation. For the detection and removal of hairs, a\n","pre-processing technique called DullRazor is used, which consists of applying a series of morphological operations\n","to the image in order to generate a mask that contains the hairs. The steps to apply the DullRazor algorithm are:\n","\n","1) Convert the original image to grayscale.\n","2) Closing to the grayscale image, using a linear or cross-shaped kernel.\n","3) Calculate the difference between the resulting image and the original.\n","4) Apply binary thresholding to obtain a mask with the hairs in the image.\n","5) Replace the pixels in common between the mask and the original image, with pixels from the latter.\n","\"\"\"\n","\n","# IMAGE ACQUISITION\n","\n","# Input image\n","path = 'Images_GDB/ISIC_0000043_thumbnail_256.jpg'\n","# Read image\n","image = cv2.imread(path, cv2.IMREAD_COLOR)\n","# Image cropping\n","img = image[30:410, 30:560]\n","\n","# DULL RAZOR (REMOVE HAIR)\n","\n","# Gray scale\n","grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","# Black hat filter\n","kernel = cv2.getStructuringElement(1, (9, 9))\n","blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n","# Gaussian filter\n","bhg = cv2.GaussianBlur(blackhat, (3, 3), cv2.BORDER_DEFAULT)\n","# Binary thresholding (MASK)\n","ret, mask = cv2.threshold(bhg, 10, 255, cv2.THRESH_BINARY)\n","# Replace pixels of the mask\n","dst = cv2.inpaint(img, mask, 6, cv2.INPAINT_TELEA)\n","\n","# Display images\n","cv2.imshow(\"Original image\", image)\n","cv2.imshow(\"Cropped image\", img)\n","cv2.imshow(\"Gray Scale image\", grayScale)\n","cv2.imshow(\"Blackhat\", blackhat)\n","cv2.imshow(\"Binary mask\", mask)\n","cv2.imshow(\"Clean image\", dst)\n","\n","cv2.waitKey()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","source":["**Shade Removal**"],"metadata":{"id":"xlMiTfQONlaG"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","\n","img = cv2.imread(path, -1)\n","\n","rgb_planes = cv2.split(img)\n","\n","result_planes = []\n","result_norm_planes = []\n","for plane in rgb_planes:\n","    dilated_img = cv2.dilate(plane, np.ones((7,7), np.uint8))\n","    bg_img = cv2.medianBlur(dilated_img, 21)\n","    diff_img = 255 - cv2.absdiff(plane, bg_img)\n","    norm_img = cv2.normalize(diff_img,None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n","    result_planes.append(diff_img)\n","    result_norm_planes.append(norm_img)\n","\n","result = cv2.merge(result_planes)\n","result_norm = cv2.merge(result_norm_planes)\n","\n","cv2.imwrite('shadows_out.png', result)\n","cv2.imwrite('shadows_out_norm.png', result_norm)"],"metadata":{"id":"o64HueebNjmI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Seuil Adaptatif pour la detection de contours**"],"metadata":{"id":"rSL3HEnaXs3D"}},{"cell_type":"code","source":["import cv2\n","\n","def detect_mole_contours(image_path):\n","    # Charger l'image en niveaux de gris\n","    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","    # Appliquer un seuillage adaptatif pour améliorer les contours\n","    _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n","\n","    # Réduire le bruit avec une ouverture morphologique\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n","    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n","\n","    # Trouver les contours dans l'image\n","    contours, _ = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Créer une copie de l'image pour dessiner les contours\n","    img_contours = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n","\n","    # Dessiner les contours sur l'image\n","    cv2.drawContours(img_contours, contours, -1, (0, 255, 0), 2)\n","\n","    # Afficher l'image originale avec les contours détectés\n","    cv2.imshow(\"Contours\", img_contours)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","\n","\n","# Exemple d'utilisation\n","image_path = \"Images_GDB/ISIC_0000063_thumbnail_256.jpg\"\n","detect_mole_contours(image_path)"],"metadata":{"id":"E-HHdCnpW2p_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Histogram Equalizer**"],"metadata":{"id":"xPeSvfWxQ3N9"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def equalize_histogram(image_path):\n","    # Charger l'image en niveaux de gris\n","    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","\n","    # Égalisation de l'histogramme\n","    equalized_img = cv2.equalizeHist(img)\n","\n","    # Calcul des histogrammes\n","    hist_img = cv2.calcHist([img], [0], None, [256], [0, 256])\n","    hist_equalized = cv2.calcHist([equalized_img], [0], None, [256], [0, 256])\n","\n","    # Afficher l'image originale et l'image égalisée côte à côte\n","    fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n","    axs[0, 0].imshow(img, cmap='gray')\n","    axs[0, 0].set_title(\"Image originale\")\n","    axs[0, 0].axis('off')\n","    axs[0, 1].plot(hist_img, color='black')\n","    axs[0, 1].set_title(\"Histogramme de l'image originale\")\n","    axs[0, 1].set_xlim([0, 256])\n","    axs[1, 0].imshow(equalized_img, cmap='gray')\n","    axs[1, 0].set_title(\"Image égalisée\")\n","    axs[1, 0].axis('off')\n","    axs[1, 1].plot(hist_equalized, color='black')\n","    axs[1, 1].set_title(\"Histogramme de l'image égalisée\")\n","    axs[1, 1].set_xlim([0, 256])\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def equalize_histogram_rgb(image_path):\n","  image = cv2.imread(image_path)\n","  img_yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n","  img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n","  img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n","\n","  cv2.imshow('avant egalisation', image)\n","  cv2.imshow('apres egalisation', img_output)\n","  cv2.waitKey(0)\n","\n","\n","\n","# Exemple d'utilisation\n","image_path = \"Images_GDB/ISIC_0000042_thumbnail_256.jpg\"\n","#equalize_histogram(image_path)\n","equalize_histogram_rgb(image_path)"],"metadata":{"id":"ShAQeNN6Q-r4","colab":{"base_uri":"https://localhost:8080/","height":322},"executionInfo":{"status":"error","timestamp":1687340071431,"user_tz":-120,"elapsed":372,"user":{"displayName":"Colin Picolet","userId":"13664194505643219229"}},"outputId":"71fe2bf9-efc0-4c16-8618-2228a8a2f813"},"execution_count":null,"outputs":[{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-f16b26ff17ac>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Images_GDB/ISIC_0000042_thumbnail_256.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#equalize_histogram(image_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mequalize_histogram_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-f16b26ff17ac>\u001b[0m in \u001b[0;36mequalize_histogram_rgb\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mequalize_histogram_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m   \u001b[0mimg_yuv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2YUV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m   \u001b[0mimg_yuv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalizeHist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_yuv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mimg_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_yuv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_YUV2BGR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Critère de beauté d'une belle image :\n","\n","1) Sans poil\n","2) Avec Beaucoup de contraste\n","3) Nette\n","\"\"\"\n","\n","\n","# IMAGE ACQUISITION\n","\n","# Input image\n","path = 'Images_GDB/ISIC_0000043_thumbnail_256.jpg'\n","# Read image\n","image = cv2.imread(path, cv2.IMREAD_COLOR)\n","# Image cropping\n","img = image[30:410, 30:560]\n","\n","def dullrazor(img):\n","\n","    # Gray scale\n","    grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","    # Black hat filter\n","    kernel = cv2.getStructuringElement(1, (9, 9))\n","    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n","    # Gaussian filter\n","    bhg = cv2.GaussianBlur(blackhat, (3, 3), cv2.BORDER_DEFAULT)\n","    # Binary thresholding (MASK)\n","    ret, mask = cv2.threshold(bhg, 10, 255, cv2.THRESH_BINARY)\n","    # Replace pixels of the mask\n","    dst = cv2.inpaint(img, mask, 6, cv2.INPAINT_TELEA)\n","\n","    # Display images\n","    #cv2.imshow(\"Original image\", image)\n","    #cv2.imshow(\"Cropped image\", img)\n","    #cv2.imshow(\"Gray Scale image\", grayScale)\n","    #cv2.imshow(\"Blackhat\", blackhat)\n","    #cv2.imshow(\"Binary mask\", mask)\n","    #cv2.imshow(\"Clean image\", dst)\n","    #cv2.waitKey()\n","    #cv2.destroyAllWindows()\n","\n","    return dst\n","\n","def equalizer(img):\n","    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    img_hsv[:, :, 0] = cv2.equalizeHist(img_hsv[:, :, 0])\n","    image_output = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)\n","    return image_output\n","\n","def contour(img):\n","\n","    # Gray\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","    # Appliquer un seuillage adaptatif pour améliorer les contours\n","    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n","\n","    # Réduire le bruit avec une ouverture morphologique\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n","    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n","\n","    # Trouver les contours dans l'image\n","    contours, _ = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Créer une copie de l'image pour dessiner les contours\n","    img_contours = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)\n","\n","    # Dessiner les contours sur l'image\n","    cv2.drawContours(img_contours, contours, -1, (0, 255, 0), 2)\n","\n","    return img_contours\n","\n","def save_result(img):\n","    cv2.imwrite('result.png', img)\n","\n","\n","def preporcess(img):\n","\n","    hairremoved = dullrazor(img)\n","    equalized = equalizer(hairremoved)\n","    final = contour(equalized)\n","    without_equalized = contour(hairremoved)\n","\n","    save_result(final)\n","    save_result(without_equalized)\n","\n","    cv2.imshow(\"Contours\", final)\n","    cv2.imshow(\"Without Equalizer\", without_equalized)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","\n","preporcess(img)"],"metadata":{"id":"Trn1aWXwV6Tx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Débruitage**"],"metadata":{"id":"heC_cKrlL1sf"}},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","from matplotlib import pyplot as plt\n","\n","# Reading image from folder where it is stored\n","img = cv2.imread('../photo hair/bruit.jpg')\n","\n","# denoising of image saving it into dst image\n","dst = cv2.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 15)\n","\n","# Plotting of source and destination image\n","plt.subplot(121), plt.imshow(img)\n","plt.subplot(122), plt.imshow(dst)\n","\n","plt.show()"],"metadata":{"id":"zyzKvMXGLmyj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Détection de symétrie**"],"metadata":{"id":"cbKs5MzLMJAg"}},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","\n","img = cv2.imread('../photo hair/as4.jpg')\n","gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","blur = cv2.GaussianBlur(gray, (5, 5), 0)\n","ret,thresh = cv2.threshold(blur,70,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n","contours,hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","max_cnt = max(contours, key=cv2.contourArea)\n","\n","ellipse = cv2.fitEllipse(max_cnt)\n","ellipse_pnts = cv2.ellipse2Poly( (int(ellipse[0][0]),int(ellipse[0][1]) ) ,( int(ellipse[1][0]),int(ellipse[1][1]) ),int(ellipse[2]),0,360,1)\n","comp = cv2.matchShapes(max_cnt,ellipse_pnts,1,0.0)\n","\n","if comp < 0.099:\n","\tprint(\"symmetric\")\n","else:\n","    print(\"asymmetric\")"],"metadata":{"id":"xm5Y5TGqMN96"},"execution_count":null,"outputs":[]}]}